{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 128, 128)\n",
      "X_train size : 100000\n",
      "X_test size : 18149\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Let's read the data\n",
    "import numpy as np\n",
    "sample = np.load('../data/x_train/0.npz')\n",
    "data = sample['data']\n",
    "print(data.shape)\n",
    "\n",
    "# Let's see how many samples we have\n",
    "print(f\"X_train size : {len(os.listdir('../data/x_train'))}\")\n",
    "print(f\"X_test size : {len(os.listdir('../data/x_test'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the first benchmark\n",
    "def benchmark(x_test_dir):\n",
    "    n_t_out, out_size = 8, 2\n",
    "    in_size = 128\n",
    "    crop = (in_size - out_size) // 2\n",
    "    benchmark_prediction = []\n",
    "    benchmark_ids = []\n",
    "    for file in os.listdir(x_test_dir):\n",
    "        x_test = np.load(f'{x_test_dir}/{file}')\n",
    "        y_bench = np.concatenate([\n",
    "            x_test['data'][-1:, crop:-crop, crop:-crop] \n",
    "            for _ in range(n_t_out)\n",
    "        ])\n",
    "        benchmark_prediction.append(y_bench.mean(axis=(1, 2)))\n",
    "        benchmark_ids.append(x_test['target_ids'])\n",
    "    return pd.DataFrame({\n",
    "        'ID': np.concatenate(benchmark_ids), \n",
    "        'TARGET': np.concatenate(benchmark_prediction)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the second benchmark\n",
    "get_method = None\n",
    "extrapolation = None\n",
    "\n",
    "def benchmark_pysteps(x_test_dir):\n",
    "    n_t_out, out_size = 8, 2\n",
    "    in_size = 128\n",
    "    crop = (in_size - out_size) // 2\n",
    "    benchmark_prediction = []\n",
    "    benchmark_ids = []\n",
    "    pysteps_flow_method = get_method('LK')\n",
    "    for file in os.listdir(x_test_dir):\n",
    "\n",
    "        x_test = np.load(f'{x_test_dir}/{file}')\n",
    "        motion_field = pysteps_flow_method(x_test['data'])\n",
    "\n",
    "        predictions = extrapolation.forecast(\n",
    "            x_test['data'][-1, ...], \n",
    "            motion_field, \n",
    "            n_t_out\n",
    "        )\n",
    "\n",
    "        predictions[np.isnan(predictions)] = 0.\n",
    "\n",
    "        benchmark_prediction.append(\n",
    "            predictions[:, crop:-crop, crop:-crop].mean(axis=(1, 2))\n",
    "        )\n",
    "\n",
    "        benchmark_ids.append(x_test['target_ids'])\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        'ID': np.concatenate(benchmark_ids), \n",
    "        'TARGET': np.concatenate(benchmark_prediction)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seem we have a have enough data to train a neural network. There is spatial and space dependancies. We can try a convolutionnal model to extract features and then a recurrent model to predict the next frame. Or we could directly use a 3D convolutionnal layer to extract spatio-temporal features. I will try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define commun class/function needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# First i need a custom dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_dir, y_dir, transform=None):\n",
    "        self.x_dir = x_dir\n",
    "        self.y = pd.read_csv(f'../data/{y_dir}.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(f'../data/{self.x_dir}'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        x = np.load(f'../data/{self.x_dir}/{idx}.npz')['data']\n",
    "        y = self.y[self.y['ID'] == idx]['TARGET'].values\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x = torch.from_numpy(x).type(torch.float32)  # Assuming x is a numpy array\n",
    "        y = torch.tensor(y, dtype=torch.float32)     # Adjust dtype as per your data\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then i need a training loop\n",
    "def train_loop(model, loss_fn, optimizer, train_loader, device):\n",
    "    size = len(train_loader.dataset)\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's validate the model with evaluation metrics : Mean Squared Logarithmic Error\n",
    "def test_loop(model, loss_fn, test_loader, device):\n",
    "    size = len(test_loader.dataset)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define loss, optimizer, device and training/testing data\n",
    "# parameters\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "def opt(model, learning_rate, N):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    print(N)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=N)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# Device\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Training data\n",
    "split = 0.8\n",
    "dataset = Dataset('x_train', 'y_train')\n",
    "\n",
    "# Split data\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * split), len(dataset) - int(len(dataset) * split)])\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a models\n",
    "# I will begin with the CNN + LSTM model. With x[data] i have 4 time steps with features of 128x128.\n",
    "# I need to use the same CNN to compute features from thoses 128x128 points.\n",
    "# Then i will use a LSTM to learn the time dependency.\n",
    "# I will then use a linear layer to predict the scalar value.\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        f1, f2, f3, f4 = 16, 16, 8, 4\n",
    "        self.conv1 = nn.Conv2d(1, f1, 3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(f1)\n",
    "        self.conv2 = nn.Conv2d(f1, f2, 3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(f2)\n",
    "        self.conv3 = nn.Conv2d(f2, f3, 3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(f3)\n",
    "        self.conv4 = nn.Conv2d(f3, f4, 3)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(f4)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(4, -1, 1, 128, 128)\n",
    "        x = [self.pool(self.relu(self.conv1(x[i]))) for i in range(4)]\n",
    "        x = [self.batchnorm1(x[i]) for i in range(4)]\n",
    "        x = [self.pool(self.relu(self.conv2(x[i]))) for i in range(4)]\n",
    "        x = [self.batchnorm2(x[i]) for i in range(4)]\n",
    "        x = [self.pool(self.relu(self.conv3(x[i]))) for i in range(4)]\n",
    "        x = [self.batchnorm3(x[i]) for i in range(4)]\n",
    "        x = [self.pool(self.relu(self.conv4(x[i]))) for i in range(4)]\n",
    "        x = [self.batchnorm4(x[i]) for i in range(4)]\n",
    "        x = torch.stack(x)\n",
    "        x = x.view(-1, 4, 144)\n",
    "        return x\n",
    "    \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(144, 64, 1, batch_first=True)\n",
    "        self.linear1 = nn.Linear(64, 32)\n",
    "        self.linear2 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.cnn = CNN()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metric for test purpose\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m RMSLELoss()\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m GRU()\n\u001b[0;32m----> 9\u001b[0m optimizer, scheduler \u001b[38;5;241m=\u001b[39m opt(model, learning_rate, \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(model, optimizer, scheduler)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "Cell \u001b[0;32mIn [57], line 14\u001b[0m, in \u001b[0;36mopt\u001b[0;34m(model, learning_rate, N)\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(N)\n\u001b[0;32m---> 14\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOneCycleLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer, scheduler\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/lr_scheduler.py:1591\u001b[0m, in \u001b[0;36mOneCycleLR.__init__\u001b[0;34m(self, optimizer, max_lr, total_steps, epochs, steps_per_epoch, pct_start, anneal_strategy, cycle_momentum, base_momentum, max_momentum, div_factor, final_div_factor, three_phase, last_epoch, verbose)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m=\u001b[39m total_steps\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(epochs, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1592\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected positive integer epochs, but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epochs))\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(steps_per_epoch, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Let's train the model\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "min_learning_rate = 1e-5\n",
    "loss_fn = RMSLELoss()\n",
    "model = GRU()\n",
    "\n",
    "optimizer, scheduler = opt(model, learning_rate, len(train_loader))\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, loss_fn, optimizer, train_loader, device)\n",
    "    test_loop(model, loss_fn, test_loader, device)\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
